{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY= 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (2.4.2)\n",
      "Requirement already satisfied: sniffio in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: certifi in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.10.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import wandb \n",
    "from tqdm import tqdm\n",
    "base = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are the ingredients listed on the product:\n",
      "\n",
      "- Enriched flour (wheat flour, niacin, reduced iron, vitamin B1 [thiamin mononitrate], vitamin B2 [riboflavin], folic acid)\n",
      "- Sugar\n",
      "- Corn syrup\n",
      "- High fructose corn syrup\n",
      "- Soybean and palm oil (with TBHQ for freshness)\n",
      "- Dextrose\n",
      "\n",
      "It also contains 2% or less of:\n",
      "\n",
      "- Cocoa processed with alkali\n",
      "- Modified corn starch\n",
      "- Salt\n",
      "- Cornstarch\n",
      "- Leavening (baking soda, sodium acid pyrophosphate, monocalcium phosphate)\n",
      "- Hydrogenated palm kernel oil\n",
      "- Sodium stearoyl lactylate\n",
      "- Gelatin\n",
      "- Color added\n",
      "- Soy lecithin\n",
      "- DATEM\n",
      "- Natural and artificial flavors\n",
      "- Xanthan gum\n",
      "- Yellow 6 Lake\n",
      "- Blue 2 Lake\n",
      "- Red 40 Lake\n",
      "- Yellow 5 Lake\n",
      "- Yellow 5\n",
      "- Red 40\n",
      "- Yellow 6\n",
      "- Blue 1\n",
      "- Carnauba wax\n",
      "- Blue 2\n",
      "\n",
      "The label also indicates that the product contains wheat and soy ingredients and is distributed by Kellogg Sales Co., Battle Creek, MI 49016. It also mentions that it contains a bioengineered food ingredient.\n",
      "./JPEG_Dataset/47.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# data path\n",
    "data_dir = base + '/JPEG_Dataset/'\n",
    "\n",
    "# use dict to create dataframe\n",
    "extracted_text = {\"img\":[], \"text\":[], \"model\":[], \"prompt\":[]}\n",
    "\n",
    "# specify prompt to generate text and model\n",
    "#prompt = \"I took a picture of the back of the product. Can you extract text from the image and return it in the OCR model's text label format? Your response should only contain the extracted text label.\"\n",
    "# model 'gpt-4-vision-preview' is able to process image\n",
    "\n",
    "prompt = \"I took a picture of the back of the product. Can you return the ingredient of the products?\"\n",
    "model = 'gpt-4-vision-preview'\n",
    "\n",
    "\n",
    "# for image_local in tqdm([os.listdir(data_dir)[0]]):\n",
    "for image_local in tqdm([os.listdir(data_dir)[0]]):\n",
    "    \n",
    "    image_url = f\"data:image/jpeg;base64,{encode_image(data_dir + image_local)}\"\n",
    "\n",
    "    client = OpenAI() \n",
    "    # client = OpenAI('OpenAI API Key here')\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4-vision-preview', #gpt-4\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": image_url}\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=800,\n",
    "        )\n",
    "        text_label = response.choices[0].message.content\n",
    "    except:\n",
    "        text_label = \" \"\n",
    "        \n",
    "\n",
    "    extracted_text[\"img\"].append(data_dir + image_local)\n",
    "    extracted_text[\"text\"].append(text_label)\n",
    "    extracted_text[\"model\"].append(model)\n",
    "    extracted_text[\"prompt\"].append(prompt)\n",
    "    # print(extracted_text)\n",
    "    print(text_label)\n",
    "    print(data_dir + image_local)\n",
    "    \n",
    "\n",
    "    # print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [27:50<00:00, 12.66s/it]\n"
     ]
    }
   ],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# data path\n",
    "data_dir = base + '/JPEG_Dataset/'\n",
    "\n",
    "# use dict to create dataframe\n",
    "extracted_text = {\"img\":[], \"text\":[], \"model\":[], \"prompt\":[]}\n",
    "\n",
    "# # specify prompt to generate text and model\n",
    "# prompt = \"I took a picture of the back of the product. Can you extract text from the image and return it in the OCR model's text label format? Your response should only contain the extracted text label.\"\n",
    "# # model 'gpt-4-vision-preview' is able to process image\n",
    "# model = 'gpt-4-vision-preview'\n",
    "\n",
    "\n",
    "# specify prompt to generate text and model\n",
    "prompt = \"I took a picture of the back of the product. Can you evaluate the image condition whether it is good or bad to see the ingredient text clearly? Just response with good or bad.\"\n",
    "# model 'gpt-4-vision-preview' is able to process image\n",
    "model = 'gpt-4-vision-preview'\n",
    "\n",
    "# for image_local in tqdm([os.listdir(data_dir)[0]]):\n",
    "for image_local in tqdm(os.listdir(data_dir)):\n",
    "    \n",
    "    image_url = f\"data:image/jpeg;base64,{encode_image(data_dir + image_local)}\"\n",
    "\n",
    "    client = OpenAI() \n",
    "    # client = OpenAI('OpenAI API Key here')\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4-vision-preview', #gpt-4\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": image_url}\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=800,\n",
    "        )\n",
    "        text_label = response.choices[0].message.content\n",
    "    except:\n",
    "        text_label = \" \"\n",
    "        \n",
    "\n",
    "    extracted_text[\"img\"].append(data_dir + image_local)\n",
    "    extracted_text[\"text\"].append(text_label)\n",
    "    extracted_text[\"model\"].append(model)\n",
    "    extracted_text[\"prompt\"].append(prompt)\n",
    "    # print(extracted_text)\n",
    "\n",
    "    # print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_numer(text):\n",
    "    \n",
    "    text = text.split(\"/\")[-1].split(\".\")[0]\n",
    "        \n",
    "    return int(text)\n",
    "\n",
    "result = pd.DataFrame(extracted_text)\n",
    "result= result.query(\"text != ' '\")\n",
    "\n",
    "result[\"img_id\"] = result[\"img\"].apply(extract_numer)\n",
    "\n",
    "result.sort_values(by = \"img_id\").to_csv(\"chatgpt4_labeling_pic_condition.csv\", index = False)\n",
    "# .sort_values(by = \"img\").to_csv(\"chatgpt4_labeling.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
