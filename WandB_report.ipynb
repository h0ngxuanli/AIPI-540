{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "# use env to input your chatgpt4 API\n",
    "\n",
    "%env OPENAI_API_KEY= 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (2.4.2)\n",
      "Requirement already satisfied: sniffio in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: certifi in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/lihongxuan/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.10.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "import wandb \n",
    "from tqdm import tqdm\n",
    "base = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use W&Büêù to create dataset report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhongxuanli\u001b[0m (\u001b[33maipi549\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/wandb/run-20240204_155000-y56c7j7r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aipi549/aipi540/runs/y56c7j7r' target=\"_blank\">extract text</a></strong> to <a href='https://wandb.ai/aipi549/aipi540' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aipi549/aipi540' target=\"_blank\">https://wandb.ai/aipi549/aipi540</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aipi549/aipi540/runs/y56c7j7r' target=\"_blank\">https://wandb.ai/aipi549/aipi540/runs/y56c7j7r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 77/132 [20:40<10:41, 11.66s/it]"
     ]
    }
   ],
   "source": [
    "with wandb.init(project = \"aipi540\", name = \"extract text\") as run:\n",
    "    \n",
    "    \n",
    "    artifact = wandb.Artifact(name=\"chatgpt4_labeling\", type=\"dataset\")\n",
    "    table = wandb.Table(columns = [\"image\", \"text_label\", \"path\", \"chatgpt_model\", \"prompt\"])\n",
    "    \n",
    "\n",
    "           \n",
    "    data_dir = base + '/JPEG_Dataset/'\n",
    "    prompt = \"I took a picture of the back of the product. Can you extract text from the image and return it in the OCR model's text label format? Your response should only contain the extracted text label.\"\n",
    "    model = 'gpt-4-vision-preview'\n",
    "\n",
    "    # for image_local in [os.listdir(data_dir)[0], os.listdir(data_dir)[1]]:\n",
    "    for image_local in tqdm(os.listdir(data_dir)):\n",
    "        \n",
    "        image_url = f\"data:image/jpeg;base64,{encode_image(data_dir + image_local)}\"\n",
    "        \n",
    "        try:\n",
    "            image_array = wandb.Image(Image.open(data_dir + image_local))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "        client = OpenAI() \n",
    "        # client = OpenAI('OpenAI API Key here')\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4-vision-preview', #gpt-4\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": image_url}\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=800,\n",
    "        )\n",
    "\n",
    "            \n",
    "        table.add_data(\n",
    "            image_array,\n",
    "            response.choices[0].message.content,\n",
    "            data_dir + image_local,\n",
    "            model,\n",
    "            prompt\n",
    "        )\n",
    "        \n",
    "    artifact.add(table, \"text lable extracted by chatgpt4\")\n",
    "    run.log_artifact(artifact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
