{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "def visualize(image):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    \n",
    "def generate_random_odd_number(min_value, max_value):\n",
    "    # Generate a random integer in the specified range\n",
    "    number = random.randint(min_value, max_value)\n",
    "\n",
    "    if number % 2 == 0:\n",
    "        number = number + 1 if number < max_value else number - 1\n",
    "    return number\n",
    "\n",
    "def get_transformed_image(image_path, seed):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    width, height = image.shape[0], image.shape[1]\n",
    "    patch_size = int(np.min([width, height]) / 1.3)\n",
    "\n",
    "    transform = A.Compose([\n",
    "        \n",
    "        A.OneOf(\n",
    "            [\n",
    "                # A.RandomCrop(height = patch_size, width = patch_size),\n",
    "                A.CenterCrop(height= patch_size, width= patch_size),\n",
    "            ],\n",
    "            p = 0.4\n",
    "        ),\n",
    "        A.CLAHE(clip_limit=2, p = 0.5),\n",
    "        A.MotionBlur(blur_limit=generate_random_odd_number(9, 27), p=0.8),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.7, contrast_limit=0.7, p=1),\n",
    "        A.ShiftScaleRotate(shift_limit=0, scale_limit=0.2, rotate_limit=180, p=0.8),\n",
    "        A.HueSaturationValue(p = 0.6)\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    augmented_image = transform(image = image)[\"image\"]\n",
    "    return augmented_image\n",
    "\n",
    "\n",
    "root_dir = Path(\"/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540\")\n",
    "\n",
    "\n",
    "for seed in range(5):\n",
    "    augmented_dataset_path = root_dir / \"Synthetic_Dataset\"\n",
    "    augmented_dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "    for ori_pic in os.listdir(root_dir / \"JPEG_Dataset\"):\n",
    "        if ori_pic != '.DS_Store':\n",
    "            augmented_image = get_transformed_image(str(root_dir / \"JPEG_Dataset\" /  ori_pic), seed)\n",
    "            cv2.imwrite(str(augmented_dataset_path / (ori_pic[:-5] + \"_\" + str(seed) + \".jpeg\")), augmented_image)\n",
    "    \n",
    "\n",
    "\n",
    "# #Image.open(\"/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/JPEG_Dataset/1.jpg\")\n",
    "\n",
    "# visualize(augmented_image)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gray(image):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    return plt.imshow(image, cmap='Greys_r')\n",
    "\n",
    "\n",
    "from skimage.filters import threshold_local\n",
    "def bw_scanner(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    T = threshold_local(gray, 21, offset = 5, method = \"gaussian\")\n",
    "    return (gray > T).astype(\"uint8\") * 255\n",
    "img = bw_scanner(cv2.imread(\"/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/Synthetic_Dataset/47_2.jpeg\"))\n",
    "\n",
    "\n",
    "plot_gray(img)\n",
    "\n",
    "output = Image.fromarray(img)\n",
    "output.save('/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "def EasyOCR(image) -> str:\n",
    "    reader = easyocr.Reader(['en']) # initialize OCR\n",
    "    result = reader.readtext(image) # input image\n",
    "    return \"\\n\".join([res[1] for res in result])\n",
    "image_path = '/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/result.png'\n",
    "img =Image.open(image_path)#cv2.imread(image_path)#\n",
    "\n",
    "\n",
    "text = EasyOCR(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the image from file\n",
    "image_path = '/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/Synthetic_Dataset/1_2.jpeg'#'/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/result.png'\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Use tesseract to do OCR on the image\n",
    "text = pytesseract.image_to_string(img)\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time taken to process the image and extract text\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "text, processing_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY= 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 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the extracted text, the potential allergy ingredients are:\n",
      "- CHEESE BLEND (PART SKIM MOZZARELLA CHEESE)\n",
      "- CALCIUM LACTATE \n",
      "\n",
      "Note: The text is not clear and may contain other potential allergens that are not easily identifiable.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import wandb \n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_local\n",
    "import torch\n",
    "import easyocr\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def img_preprocess(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    T = threshold_local(gray, 21, offset = 5, method = \"gaussian\")\n",
    "    return (gray > T).astype(\"uint8\") * 255\n",
    "\n",
    "def llm_postprocess(ocr_text):\n",
    "    prompt = \"\"\"\n",
    "            Here is the extracted text from a product' ingredient part. \n",
    "            Could you please identify the ingredients and return the potential allergy ingredient? \n",
    "            Your return only includes the alleries with bullet points.\n",
    "            \"\"\"\n",
    "    client = OpenAI() \n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4-vision-preview', #gpt-4\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"text\", \"text\": ocr_text},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    allergies = response.choices[0].message.content\n",
    "    \n",
    "    return allergies\n",
    "\n",
    "\n",
    "def easyocr_model(image_path, gpu):\n",
    "    reader = easyocr.Reader(['en'])#, gpu=gpu) # initialize OCR\n",
    "    image = img_preprocess(image_path)\n",
    "    # reader = easyocr.Reader(['en'], detection='DB', recognition = 'Transformer')\n",
    "    result = reader.readtext(image) # input image\n",
    "    ocr_text =  \"\\n\".join([res[1] for res in result])\n",
    "    allergies = llm_postprocess(ocr_text)\n",
    "    return allergies\n",
    "\n",
    "def tesseract3_model(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = img_preprocess(image_path)\n",
    "    # Use tesseract to do OCR on the image\n",
    "    ocr_text = pytesseract.image_to_string(image, config=r'--oem 0')\n",
    "    \n",
    "    allergies = llm_postprocess(ocr_text)\n",
    "    return allergies\n",
    "\n",
    "# def mmocr_model(image_path):\n",
    "    \n",
    "    \n",
    "# def kerasocr_model(image_path):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def chatgpt_model(image_path):\n",
    "    prompt = \"\"\"\n",
    "            Here is the picture from a product' ingredient part. \n",
    "            Could you please identify the ingredients and return the potential allergy ingredient? \n",
    "            Your return only includes the alleries with bullet points.\n",
    "            \"\"\"\n",
    "    image_url = f\"data:image/jpeg;base64,{encode_image(image_path)}\"\n",
    "\n",
    "    client = OpenAI() \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4-vision-preview', #gpt-4\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": image_url}\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    allergies = response.choices[0].message.content\n",
    "    return allergies\n",
    "    \n",
    "# print(llm_postprocess(text))\n",
    "\n",
    "image_path = \"/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/JPEG_Dataset/9.jpg\"\n",
    "\n",
    "print(easyocr_model(image_path=image_path, gpu=False))\n",
    "\n",
    "# print(\"-\"*20)\n",
    "\n",
    "# print(tesseract3_model(image_path=image_path))\n",
    "# print(\"-\"*20)\n",
    "\n",
    "# print(chatgpt_model(image_path=image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_s/sgnns_xx60vdrf8dbqgsl7400000gn/T/ipykernel_2963/2510000288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmmocr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmocr'"
     ]
    }
   ],
   "source": [
    "import mmocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmocr.utils.ocr import MMOCR\n",
    "mmocr = MMOCR(det='TextSnake', recog='SAR', kie='SDMGR')\n",
    "mmocr.readtext('demo/demo_kie.jpeg', print_result=True, output='outputs/demo_kie_pred.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# Paddleocr目前支持的多语言语种可以通过修改lang参数进行切换\n",
    "# 例如`ch`, `en`, `fr`, `german`, `korean`, `japan`\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\")  # need to run only once to download and load model into memory\n",
    "img_path = \"/Users/lihongxuan/Desktop/AIPI/Courses/AIPI540/AIPI-540/JPEG_Dataset/9.jpg\"\n",
    "result = ocr.ocr(img_path, cls=True)\n",
    "for idx in range(len(result)):\n",
    "    res = result[idx]\n",
    "    for line in res:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import wandb \n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_local\n",
    "import torch\n",
    "import easyocr\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "# %env OPENAI_API_KEY= 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 \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def img_preprocess(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    T = threshold_local(gray, 21, offset = 5, method = \"gaussian\")\n",
    "    return (gray > T).astype(\"uint8\") * 255\n",
    "\n",
    "def llm_postprocess(ocr_text):\n",
    "    prompt = \"\"\"\n",
    "            Here is the extracted text from a product's ingredient part. \n",
    "            Could you please identify the ingredients and return the potential allergy ingredient? \n",
    "            Your return only includes the alleries with bullet points.\n",
    "            \"\"\"\n",
    "    client = OpenAI() \n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4-vision-preview', #gpt-4\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"text\", \"text\": ocr_text},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    allergies = response.choices[0].message.content\n",
    "    return allergies\n",
    "\n",
    "\n",
    "def easyocr_model(image_path, gpu):\n",
    "    reader = easyocr.Reader(['en'], gpu=gpu) # initialize OCR\n",
    "    result = reader.readtext(image_path) # input image\n",
    "    ocr_text =  \"\\n\".join([res[1] for res in result])\n",
    "    allergies = llm_postprocess(ocr_text)\n",
    "    return ocr_text, allergies\n",
    "\n",
    "def easyocr_model_w_pre(image_path, gpu):\n",
    "    reader = easyocr.Reader(['en'], gpu=gpu) # initialize OCR\n",
    "    image = img_preprocess(image_path)\n",
    "    result = reader.readtext(image) # input image\n",
    "    ocr_text =  \"\\n\".join([res[1] for res in result])\n",
    "    allergies = llm_postprocess(ocr_text)\n",
    "    return ocr_text, allergies\n",
    "\n",
    "def tesseract_model(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    allergies = llm_postprocess(ocr_text)\n",
    "    return ocr_text, allergies\n",
    "\n",
    "def tesseract_model_w_pre(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = img_preprocess(image_path)\n",
    "    # Use tesseract to do OCR on the image\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    allergies = llm_postprocess(ocr_text)\n",
    "    return ocr_text, allergies\n",
    "\n",
    "# def mmocr_model(image_path):\n",
    "    \n",
    "    \n",
    "# def kerasocr_model(image_path):\n",
    "\n",
    "\n",
    "def chatgpt_model(image_path):\n",
    "    prompt = \"\"\"\n",
    "            Here is the picture from a product' ingredient part. \n",
    "            Could you please identify the ingredients and return the potential allergy ingredient? \n",
    "            Your return only includes the alleries with bullet points.\n",
    "            \"\"\"\n",
    "    image_url = f\"data:image/jpeg;base64,{encode_image(image_path)}\"\n",
    "\n",
    "    client = OpenAI() \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4-vision-preview', #gpt-4\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": image_url}\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    allergies = response.choices[0].message.content\n",
    "    return allergies\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "data_dir = \"/home/featurize/work/OCR_exper/JPEG_Dataset_synthetic/\"\n",
    "for image_local in tqdm(os.listdir(data_dir)[]):\n",
    "    \n",
    "    try:\n",
    "        image_array = wandb.Image(Image.open(data_dir + image_local))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    image_path  = data_dir + image_local\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(image_path)\n",
    "    easyocr_text, easyocr_result = easyocr_model(image_path=image_path, gpu=True)\n",
    "    end_time = time.time()\n",
    "    easyocr_time = end_time - start_time\n",
    "    print(easyocr_result)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    easyocr_w_pre_text, easyocr_w_pre_result = easyocr_model_w_pre(image_path=image_path, gpu=True)\n",
    "    end_time = time.time()\n",
    "    easyocr_w_pre_time = end_time - start_time\n",
    "    print(easyocr_w_pre_result)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    tesseract_text, tesseract_result = tesseract_model(image_path=image_path)\n",
    "    end_time = time.time()\n",
    "    tesseract_time = end_time - start_time\n",
    "    print(tesseract_result)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    tesseract_w_pre_text, tesseract_w_pre_result = tesseract_model_w_pre(image_path=image_path)\n",
    "    end_time = time.time()\n",
    "    tesseract_w_pre_time = end_time - start_time\n",
    "    print(tesseract_w_pre_result)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    chatgpt_result = chatgpt_model(image_path=image_path)\n",
    "    end_time = time.time()\n",
    "    chatgpt_time = end_time - start_time\n",
    "    print(chatgpt_result)\n",
    "    print(\"\\n\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
